{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q -U bitsandbytes\n!pip install -q -U git+https://github.com/huggingface/transformers.git\n!pip install -q -U git+https://github.com/huggingface/peft.git\n!pip install -q -U git+https://github.com/huggingface/accelerate.git\n!pip install -q datasets\n!pip install -q scipy\n!pip install -U accelerate","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-23T20:34:30.203897Z","iopub.execute_input":"2024-05-23T20:34:30.204483Z","iopub.status.idle":"2024-05-23T20:37:08.403186Z","shell.execute_reply.started":"2024-05-23T20:34:30.204453Z","shell.execute_reply":"2024-05-23T20:37:08.402183Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.31.0.dev0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.23.1)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.2.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom peft import PeftModel, PeftConfig\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n\n# Load the PEFT configuration\nconfig = PeftConfig.from_pretrained(\"DucPhanBa/Vietnamese_Llama2\")\n\n# Load the base model and move it to the device\nbase_model = AutoModelForCausalLM.from_pretrained(\"guardrail/llama-2-7b-guanaco-instruct-sharded\")\n\n# Load the PEFT model and move it to the device\nmodel = PeftModel.from_pretrained(base_model, \"DucPhanBa/Vietnamese_Llama2\")\n\n# Load the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"guardrail/llama-2-7b-guanaco-instruct-sharded\")\n\n# Define the input text\ninput_text = \"Viết 1 đoạn code python in từ 1 tới 100\"\n\n# Tokenize the input text and move it to the device\ninputs = tokenizer(input_text, return_tensors=\"pt\")\n\n# Generate response with max_new_tokens set\noutputs = model.generate(**inputs, max_new_tokens=100)\n\n# Decode and print the generated response (moving the output back to CPU for decoding)\nresponse = tokenizer.decode(outputs[0].cpu(), skip_special_tokens=True)\nprint(response)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-23T20:39:11.316735Z","iopub.execute_input":"2024-05-23T20:39:11.317566Z","iopub.status.idle":"2024-05-23T20:48:20.912003Z","shell.execute_reply.started":"2024-05-23T20:39:11.317524Z","shell.execute_reply":"2024-05-23T20:48:20.910905Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"adapter_config.json:   0%|          | 0.00/467 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f8e7e6dc3c34d3eb514bce0faa9e977"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/633 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"accd4ede36714907ac511551ed9ddc1d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6b37d210ee744818d6bc90c950fcb2d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/14 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a69b8f4c5b844c39af9c845138e42ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00014.safetensors:   0%|          | 0.00/1.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b3f81bb209c45dbba984a68da3f66ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00014.safetensors:   0%|          | 0.00/1.93G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"811adf6a9b4a4f24988e79015a341d18"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00014.safetensors:   0%|          | 0.00/1.93G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aeea4cb4aa83433abc291db1e583433b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00014.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c37396f10c454ddf8b996f286c1491bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00005-of-00014.safetensors:   0%|          | 0.00/1.89G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7e1852c34904c6ab5ca61ed237fa7f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00006-of-00014.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2dc195d169874e6aa871d8eb10951a97"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00007-of-00014.safetensors:   0%|          | 0.00/1.93G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02c33b41ea934019b0cdb629af2f14a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00008-of-00014.safetensors:   0%|          | 0.00/1.93G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a9e27432a1d47fd89739a015069b4b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00009-of-00014.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69132208384a43bf9fe0b5a099ace88d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00010-of-00014.safetensors:   0%|          | 0.00/1.89G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cdf8db470c474cc88c9d464737de111e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00011-of-00014.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95a855b49e92401a8dd999b868d320b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00012-of-00014.safetensors:   0%|          | 0.00/1.93G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c77de6b4a5041079878941d03d5eb5a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00013-of-00014.safetensors:   0%|          | 0.00/1.93G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9cced29f336c41f4abfff9a278d33bf9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00014-of-00014.safetensors:   0%|          | 0.00/1.69G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ace4cffe13a48b38f7479210689667c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db2ae04c52bf4cdab58e85f188100b81"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20bbe2367a214244b55e50ad88f2dbe4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.bin:   0%|          | 0.00/134M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a5608108526409e8a1af15166004974"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/676 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4909fe063ab74e21ace285de89516f3c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13fc008b116b401d975c7a35f0c04554"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/434 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a6dff573b044aeeb08e499b3886e878"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","output_type":"stream"},{"name":"stdout","text":"Viết 1 đoạn code python in từ 1 tới 100.\n\nĐoạn code Python sau đây cho phép bạn viết một đoạn code từ 1 tới 100:\n\n```\nfor i in range(1, 101):\n    print(i)\n```\n\nBạn có thể thêm vào các thuộc tính hoặc thuộ\n","output_type":"stream"}]},{"cell_type":"code","source":"input_text = \"Dịch sư tử sang tiếng nga\"\n\n# Tokenize the input text\ninputs = tokenizer(input_text, return_tensors=\"pt\")\n\n# Generate response with max_new_tokens set\noutputs = model.generate(**inputs, max_new_tokens=100)\n\n# Decode and print the generated response\nresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(response)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T18:56:56.871636Z","iopub.execute_input":"2024-05-23T18:56:56.872083Z","iopub.status.idle":"2024-05-23T18:58:55.928417Z","shell.execute_reply.started":"2024-05-23T18:56:56.872056Z","shell.execute_reply":"2024-05-23T18:58:55.927426Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Dịch sư tử sang tiếng nga\n\nTừ 'sư tử' trong tiếng Anh được dịch sang tiếng Nga là 'лев'.\n\nTừ 'sư tử' trong tiếng Anh có nghĩa là 'người đàn ông', 'người đàn bà' hoặc 'người \n","output_type":"stream"}]},{"cell_type":"code","source":"input_text = \"Đề xuất 1 cuốn sách hay\"\n\n# Tokenize the input text\ninputs = tokenizer(input_text, return_tensors=\"pt\")\n\n# Generate response with max_new_tokens set\noutputs = model.generate(**inputs, max_new_tokens=200)\n\n# Decode and print the generated response\nresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(response)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T18:58:55.930922Z","iopub.execute_input":"2024-05-23T18:58:55.931306Z","iopub.status.idle":"2024-05-23T19:02:54.091533Z","shell.execute_reply.started":"2024-05-23T18:58:55.931269Z","shell.execute_reply":"2024-05-23T19:02:54.090348Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Đề xuất 1 cuốn sách hay phim để đọc hoặc xem nào?\n\nNhà văn: 'The Alchemist' của Paulo Coelho là một cuốn sách đáng đọc về tình yêu, niềm vui và niềm tin. Nó kể về chuyến đi của một chàng trai trẻ tên là Santiago, người đi tìm cái may mắn của mình và tìm kiếm niềm vui và niềm tin của mình. Cuốn sách đã thu hút tôi bằng cách kể về những khó kh\n","output_type":"stream"}]},{"cell_type":"code","source":"input_text = \"Ăn gì để khỏe mạnh?\"\n\n# Tokenize the input text\ninputs = tokenizer(input_text, return_tensors=\"pt\")\n\n# Generate response with max_new_tokens set\noutputs = model.generate(**inputs, max_new_tokens=200)\n\n# Decode and print the generated response\nresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(response)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T19:02:54.092774Z","iopub.execute_input":"2024-05-23T19:02:54.093067Z","iopub.status.idle":"2024-05-23T19:06:51.860279Z","shell.execute_reply.started":"2024-05-23T19:02:54.093040Z","shell.execute_reply":"2024-05-23T19:06:51.859270Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Ăn gì để khỏe mạnh?\n - Đồ ăn chứa chất béo và chất béo không tốt cho sức khỏe.\n - Đồ ăn chứa chất béo và chất béo không tốt cho sức khỏe.\n - Đồ ăn chứa chất béo và chất béo không tốt cho sức khỏe.\n - Đồ ăn chứa chất béo và chất béo không tốt cho sức khỏe.\n - Đồ ăn chứa chất béo và chất béo không tốt cho sức kh\n","output_type":"stream"}]},{"cell_type":"code","source":"input_text = \"Có nên làm việc online không?\"\n\n# Tokenize the input text\ninputs = tokenizer(input_text, return_tensors=\"pt\")\n\n# Generate response with max_new_tokens set\noutputs = model.generate(**inputs, max_new_tokens=200)\n\n# Decode and print the generated response\nresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(response)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T19:06:51.861648Z","iopub.execute_input":"2024-05-23T19:06:51.862052Z","iopub.status.idle":"2024-05-23T19:10:49.651897Z","shell.execute_reply.started":"2024-05-23T19:06:51.862015Z","shell.execute_reply":"2024-05-23T19:10:49.650951Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Có nên làm việc online không?\n\nCó nhiều lợi ích khi làm việc online, bao gồm:\n\n1. Tự do và tự do: Bạn có thể làm việc từ bất cứ nơi nào, không cần phải đi đến nơi làm việc.\n2. Tăng thời gian cho bạn: Bạn có thể làm việc khi bạn muốn, không cần phải đi làm việc vào lúc đã định.\n3. Tăng khả năng làm việc: Bạn có thể làm việc ở nơi k\n","output_type":"stream"}]},{"cell_type":"code","source":"input_text = \"Viết 1 câu chia buồn với người bạn thân\"\n\n# Tokenize the input text\ninputs = tokenizer(input_text, return_tensors=\"pt\")\n\n# Generate response with max_new_tokens set\noutputs = model.generate(**inputs, max_new_tokens=200)\n\n# Decode and print the generated response\nresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(response)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T19:10:49.653325Z","iopub.execute_input":"2024-05-23T19:10:49.653828Z","iopub.status.idle":"2024-05-23T19:14:48.807090Z","shell.execute_reply.started":"2024-05-23T19:10:49.653791Z","shell.execute_reply":"2024-05-23T19:14:48.805920Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Viết 1 câu chia buồn với người bạn thân của bạn.\n\nBạn có thể viết một câu chia buồn cho người bạn thân của bạn khi bạn đang trải qua một thời kỳ khó khăn. Câu chia buồn này có thể giúp người bạn thân của bạn cảm thấy được hỗ trợ và được đón nhận.\n\nCâu chia buồn này có thể là:\n\n'Bạn đang trải qua một thời kỳ khó khăn, nhưng bạn không phải\n","output_type":"stream"}]},{"cell_type":"code","source":"input_text = \" Thủ đô của Việt Nam là gì? \"\n\n# Tokenize the input text\ninputs = tokenizer(input_text, return_tensors=\"pt\")\n\n# Generate response with max_new_tokens set\noutputs = model.generate(**inputs, max_new_tokens=200)\n\n# Decode and print the generated response\nresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(response)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T19:14:48.808433Z","iopub.execute_input":"2024-05-23T19:14:48.808798Z","iopub.status.idle":"2024-05-23T19:18:47.004038Z","shell.execute_reply.started":"2024-05-23T19:14:48.808737Z","shell.execute_reply":"2024-05-23T19:18:47.002964Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":" Thủ đô của Việt Nam là gì? \n - Thủ đô của Việt Nam là Hà Nội.\n\nHà Nội là thủ đô của Việt Nam từ năm 1975, khi Việt Nam thống nhất sau khi chấm dứt chiến tranh chống Mỹ. Thủ đô của Việt Nam trước đó là Huế, nhưng sau khi Việt Nam thống nhất, chính phủ Việt Nam đã chuyển thủ đô từ Huế đến Hà Nội.\n\nHà Nội là thủ đô của Việt Nam và là thành phố lớn nhấ\n","output_type":"stream"}]},{"cell_type":"code","source":"input_text = \" Trái đất có bao nhiêu châu lục?  \"\n\n# Tokenize the input text\ninputs = tokenizer(input_text, return_tensors=\"pt\")\n\n# Generate response with max_new_tokens set\noutputs = model.generate(**inputs, max_new_tokens=200)\n\n# Decode and print the generated response\nresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(response)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T19:18:47.005662Z","iopub.execute_input":"2024-05-23T19:18:47.006071Z","iopub.status.idle":"2024-05-23T19:22:45.795754Z","shell.execute_reply.started":"2024-05-23T19:18:47.006032Z","shell.execute_reply":"2024-05-23T19:22:45.794500Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":" Trái đất có bao nhiêu châu lục?  \n\n1. 7\n2. 10\n3. 12\n4. 15\n\n answer: 7\n\n Tính từ đầu ra, châu lục trái đất có 7 châu lục.\n\n1. Bắc Mỹ\n2. Nam Mỹ\n3. Châu Âu\n4. Châu Á\n5. Châu Phi\n6. Australia\n7. New Zealand\n\n answer: 7\n\n Tính từ đầu ra, châu lục trái đất có 7 châu lục.\n\n1. Bắc Mỹ\n2. Nam Mỹ\n3. Châu Âu\n4. Châu Á\n5. Châu Phi\n\n","output_type":"stream"}]},{"cell_type":"code","source":"input_text = \" Ai là người phát minh ra bóng đèn?  \"\n\n# Tokenize the input text\ninputs = tokenizer(input_text, return_tensors=\"pt\")\n\n# Generate response with max_new_tokens set\noutputs = model.generate(**inputs, max_new_tokens=200)\n\n# Decode and print the generated response\nresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(response)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T19:22:45.800105Z","iopub.execute_input":"2024-05-23T19:22:45.800451Z","iopub.status.idle":"2024-05-23T19:26:43.416118Z","shell.execute_reply.started":"2024-05-23T19:22:45.800426Z","shell.execute_reply":"2024-05-23T19:26:43.415148Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":" Ai là người phát minh ra bóng đèn?  \n\nBóng đèn được phát minh bởi Humphry Davy vào năm 1809, và được sử dụng rộng rãi trong các năm 1810 và 1811. Nó được sử dụng để cung cấp ánh sáng cho các nhà hát và các sự kiện công cộng, và sau đó đã trở thành một phần của cuộc sống hàng ngày.\n\nNhà khoa học người Anh Humphry Davy đã phát minh ra bó\n","output_type":"stream"}]},{"cell_type":"code","source":"input_text = \"Nước biển có vị gì?\"\n\n# Tokenize the input text\ninputs = tokenizer(input_text, return_tensors=\"pt\")\n\n# Generate response with max_new_tokens set\noutputs = model.generate(**inputs, max_new_tokens=200)\n\n# Decode and print the generated response\nresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(response)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T19:26:43.417348Z","iopub.execute_input":"2024-05-23T19:26:43.417680Z","iopub.status.idle":"2024-05-23T19:30:43.602081Z","shell.execute_reply.started":"2024-05-23T19:26:43.417651Z","shell.execute_reply":"2024-05-23T19:30:43.601071Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Nước biển có vị gì?\n\nNước biển có vị ngọt và có một số độc đáo của riêng nó. Nước biển là một trong những nguồn nước tự nhiên quan trọng nhất trên Trái Đất, và nó có một số đặc điểm về vị trí, động lực học và các chất hóa học.\n\nVị ngọt của nước biển là do sự hiện diện của muối, một chất hòa tan trong nước. Muối là một chấ\n","output_type":"stream"}]},{"cell_type":"code","source":"input_text = \"Mặt trời mọc ở hướng nào? \"\n\n# Tokenize the input text\ninputs = tokenizer(input_text, return_tensors=\"pt\")\n\n# Generate response with max_new_tokens set\noutputs = model.generate(**inputs, max_new_tokens=200)\n\n# Decode and print the generated response\nresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(response)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T19:30:43.603389Z","iopub.execute_input":"2024-05-23T19:30:43.603933Z","iopub.status.idle":"2024-05-23T19:34:42.764451Z","shell.execute_reply.started":"2024-05-23T19:30:43.603898Z","shell.execute_reply":"2024-05-23T19:34:42.763414Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Mặt trời mọc ở hướng nào? Điều này có thể ảnh hưởng đến cách mà bạn đối diện với ngày và những thay đổi trong năng lượng của nó.\n\nMặt trời mọc ở hướng đông vào buổi sáng và mặt trời mặt trời mặt trời mặt trời mặt trời mặt trời mặt trời mặt trời mặt trời mặt trời mặt trời m\n","output_type":"stream"}]},{"cell_type":"code","source":"input_text = \"Quốc khánh Việt Nam là ngày nào?\"\n\n# Tokenize the input text\ninputs = tokenizer(input_text, return_tensors=\"pt\")\n\n# Generate response with max_new_tokens set\noutputs = model.generate(**inputs, max_new_tokens=200)\n\n# Decode and print the generated response\nresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(response)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T19:34:42.765816Z","iopub.execute_input":"2024-05-23T19:34:42.766137Z","iopub.status.idle":"2024-05-23T19:38:41.266021Z","shell.execute_reply.started":"2024-05-23T19:34:42.766111Z","shell.execute_reply":"2024-05-23T19:38:41.265056Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Quốc khánh Việt Nam là ngày nào?\n\nQuốc khánh Việt Nam là ngày 2 tháng 9, kỷ niệm ngày Việt Nam độc lập vào năm 1945. Ngày này được chính phủ Việt Nam thông qua vào năm 1945 và được tổ chức lần đầu tiên vào năm 1946. Ngày này được tổ chức hàng năm và được coi là ngày lễ quốc gia của Việt Nam.\n\nNgày này được tổ ch\n","output_type":"stream"}]},{"cell_type":"code","source":"input_text = \"Thế vận hội Olympic diễn ra bao nhiêu năm một lần?\"\n\n# Tokenize the input text\ninputs = tokenizer(input_text, return_tensors=\"pt\")\n\n# Generate response with max_new_tokens set\noutputs = model.generate(**inputs, max_new_tokens=200)\n\n# Decode and print the generated response\nresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(response)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T19:38:41.267321Z","iopub.execute_input":"2024-05-23T19:38:41.267661Z","iopub.status.idle":"2024-05-23T19:42:40.688452Z","shell.execute_reply.started":"2024-05-23T19:38:41.267633Z","shell.execute_reply":"2024-05-23T19:42:40.687418Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Thế vận hội Olympic diễn ra bao nhiêu năm một lần?\n\nThế vận hội Olympic diễn ra mỗi 4 năm một lần.\n\nCuộc thi Olympic đầu tiên được tổ chức vào năm 1896 tại Athens, Hy Lạp.\n\nCuộc thi Olympic được tổ chức đầu tiên được tổ chức vào năm 1896 tại Athens, Hy Lạp.\n\nCuộc thi Olympic được tổ chức đầu tiên được tổ chức vào năm 1896 tại Athens, Hy L\n","output_type":"stream"}]},{"cell_type":"code","source":"input_text = \"Cúp bóng đá thế giới gần nhất được tổ chức ở đâu?\"\n\n# Tokenize the input text\ninputs = tokenizer(input_text, return_tensors=\"pt\")\n\n# Generate response with max_new_tokens set\noutputs = model.generate(**inputs, max_new_tokens=200)\n\n# Decode and print the generated response\nresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(response)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T19:42:40.689572Z","iopub.execute_input":"2024-05-23T19:42:40.689885Z","iopub.status.idle":"2024-05-23T19:46:42.086090Z","shell.execute_reply.started":"2024-05-23T19:42:40.689859Z","shell.execute_reply":"2024-05-23T19:46:42.085085Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Cúp bóng đá thế giới gần nhất được tổ chức ở đâu?\n\nCúp bóng đá thế giới 2018 được tổ chức ở Nga.\n\nCuộc thi bóng đá thế giới được tổ chức bởi FIFA, và là giải đấu bóng đá lớn nhất và được ưa chuộng nhất trên thế giới.\n\nCúp bóng đá thế giới được tổ chức mỗi năm, và đây là danh hiệu cao nhất của bóng đá thế giới.\n\nC\n","output_type":"stream"}]},{"cell_type":"code","source":"input_text = \"Ai là tổng thống hiện tại của Hoa Kỳ?\"\n\n# Tokenize the input text\ninputs = tokenizer(input_text, return_tensors=\"pt\")\n\n# Generate response with max_new_tokens set\noutputs = model.generate(**inputs, max_new_tokens=200)\n\n# Decode and print the generated response\nresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(response)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T19:46:42.087610Z","iopub.execute_input":"2024-05-23T19:46:42.087927Z","iopub.status.idle":"2024-05-23T19:49:11.181001Z","shell.execute_reply.started":"2024-05-23T19:46:42.087900Z","shell.execute_reply":"2024-05-23T19:49:11.180024Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Ai là tổng thống hiện tại của Hoa Kỳ?\n\nAi là tổng thống hiện tại của Hoa Kỳ?\n\n answer: Joe Biden\n\nTổng thống hiện tại của Hoa Kỳ là Joe Biden, người đã đắc cử vào năm 2020 và đã nhậm chức vào ngày 20 tháng 1 năm 2021.\n","output_type":"stream"}]},{"cell_type":"code","source":"input_text = \"Viết một hàm Python in ra Hello, World!\"\n\n# Tokenize the input text\ninputs = tokenizer(input_text, return_tensors=\"pt\")\n\n# Generate response with max_new_tokens set\noutputs = model.generate(**inputs, max_new_tokens=200)\n\n# Decode and print the generated response\nresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(response)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T20:54:40.330607Z","iopub.execute_input":"2024-05-23T20:54:40.331670Z","iopub.status.idle":"2024-05-23T20:58:38.246080Z","shell.execute_reply.started":"2024-05-23T20:54:40.331634Z","shell.execute_reply":"2024-05-23T20:58:38.245142Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Viết một hàm Python in ra Hello, World!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","output_type":"stream"}]},{"cell_type":"code","source":"input_text = \"Làm thế nào để cộng hai số trong Python? \"\n\n# Tokenize the input text\ninputs = tokenizer(input_text, return_tensors=\"pt\")\n\n# Generate response with max_new_tokens set\noutputs = model.generate(**inputs, max_new_tokens=200)\n\n# Decode and print the generated response\nresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(response)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T20:58:38.248133Z","iopub.execute_input":"2024-05-23T20:58:38.248440Z","iopub.status.idle":"2024-05-23T21:02:35.076654Z","shell.execute_reply.started":"2024-05-23T20:58:38.248414Z","shell.execute_reply":"2024-05-23T21:02:35.075750Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Làm thế nào để cộng hai số trong Python? \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","output_type":"stream"}]},{"cell_type":"code","source":"input_text = \"HTML là viết tắt của gì?\"\n\n# Tokenize the input text\ninputs = tokenizer(input_text, return_tensors=\"pt\")\n\n# Generate response with max_new_tokens set\noutputs = model.generate(**inputs, max_new_tokens=200)\n\n# Decode and print the generated response\nresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(response)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T21:02:35.078281Z","iopub.execute_input":"2024-05-23T21:02:35.078993Z","iopub.status.idle":"2024-05-23T21:06:32.343714Z","shell.execute_reply.started":"2024-05-23T21:02:35.078954Z","shell.execute_reply":"2024-05-23T21:06:32.342787Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"HTML là viết tắt của gì?\n\nHTML là viết tắt của HyperText Markup Language.\n\nHTML là ngôn ngữ mã hóa được sử dụng để xác định cấu trúc của một trang web, bao gồm các thành phần như các bài viết, hình ảnh, danh mục, liên kết và các thành phần khác. HTML là ngôn ngữ mã hóa được sử dụng rộng rãi nhất để xây dựng các trang web.\n\nHTML được sử dụng cùng với\n","output_type":"stream"}]},{"cell_type":"code","source":"input_text = \"Hãy hướng dẫn cách làm một chiếc bánh mì kẹp\"\n\n# Tokenize the input text\ninputs = tokenizer(input_text, return_tensors=\"pt\")\n\n# Generate response with max_new_tokens set\noutputs = model.generate(**inputs, max_new_tokens=200)\n\n# Decode and print the generated response\nresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(response)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T21:06:32.346338Z","iopub.execute_input":"2024-05-23T21:06:32.346618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_text = \"Làm thế nào để làm bài tập về nhà? \"\n\n# Tokenize the input text\ninputs = tokenizer(input_text, return_tensors=\"pt\")\n\n# Generate response with max_new_tokens set\noutputs = model.generate(**inputs, max_new_tokens=200)\n\n# Decode and print the generated response\nresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(response)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_text = \"Làm thế nào để gửi một email?\"\n\n# Tokenize the input text\ninputs = tokenizer(input_text, return_tensors=\"pt\")\n\n# Generate response with max_new_tokens set\noutputs = model.generate(**inputs, max_new_tokens=200)\n\n# Decode and print the generated response\nresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(response)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_text = \"Hãy hướng dẫn cách gấp một con hạc giấy.\"\n\n# Tokenize the input text\ninputs = tokenizer(input_text, return_tensors=\"pt\")\n\n# Generate response with max_new_tokens set\noutputs = model.generate(**inputs, max_new_tokens=300)\n\n# Decode and print the generated response\nresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(response)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_text = \"Làm sao để khỏe mạnh?\"\n\n# Tokenize the input text\ninputs = tokenizer(input_text, return_tensors=\"pt\")\n\n# Generate response with max_new_tokens set\noutputs = model.generate(**inputs, max_new_tokens=300)\n\n# Decode and print the generated response\nresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(response)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}
